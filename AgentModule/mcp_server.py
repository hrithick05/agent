"""
FastMCP Server for Web Scraping Bot
Converts all scraping tools into MCP-compatible tools for LLM usage
"""

from fastmcp import FastMCP
from typing import Dict, List, Any, Optional, Union
import os

# Import our existing modules
from summaryModulle.fetchHTML import fetch_html
from summaryModulle.main import analyze_html
from SelectorToDB.generic_scraper import GenericPlatformScraper
# Import only the SelectorAnalyzer class
from SelectorToDB.data_analysis import SelectorAnalyzer

# Initialize FastMCP server
mcp = FastMCP("Web Scraping Bot ðŸ•·ï¸")

# Global state management - accessible by all tools
html_content: str = ""
summary: Dict[str, Any] = {}
GenericPlatformScraperObj: GenericPlatformScraper = None

# Global selector template - empty by default, to be filled by LLM incrementally
selector_template: Dict[str, Any] = {
    'product_container': {
        'type': None,
        'selectors': []
    },
    'name': {
        'type': None,
        'selectors': []
    },
    'current_price': {
        'type': None,
        'selectors': []
    },
    'original_price': {
        'type': None,
        'selectors': []
    },
    'rating': {
        'type': None,
        'selectors': []
    },
    'reviews': {
        'type': None,
        'selectors': []
    },
    'discount': {
        'type': None,
        'selectors': []
    },
    'offers': {
        'type': None,
        'selectors': []
    }
}

# ============================================================================
# CORE HTML AND ANALYSIS TOOLS
# ============================================================================

@mcp.tool
async def get_html(url: str) -> Dict[str, str]:
    """
    Fetch HTML content from URL and perform comprehensive analysis.
    
    This function fetches HTML content using Crawl4AI and analyzes it to extract
    structural patterns, field hints, and metadata that can be used for scraping.
    
    Args:
        url (str): The URL to fetch HTML content from
        
    Returns:
        Dict[str, str]: Success message confirming content saved in global variables
        
    Side Effects:
        - Updates global html_content variable with fetched HTML
        - Updates global summary variable with analysis results
    """
    try:
        global html_content, summary
        html_content = await fetch_html(url)
        summary = analyze_html(html_content)
        return {"message": "Successfully fetched and analyzed HTML content", "status": "success"}
    except Exception as e:
        return {"message": f"Error fetching HTML: {str(e)}", "status": "error"}

@mcp.tool
def readsummary(field: Optional[str] = None) -> Dict[str, Any]:
    """
    Read analysis summary data for a specific field or return all available fields.
    
    This function provides access to the HTML analysis results including:
    - DOM structure analysis
    - Repeated pattern detection
    - Field hint generation
    - Text pattern recognition
    - Form analysis
    - Confidence scoring
    
    Args:
        field (Optional[str]): Specific field name to retrieve. If None, returns all available field names.
                              Common fields: 'repeats', 'field_hint_map', 'text_patterns', 
                              'confidence_summary', 'forms_detailed', etc.
    
    Returns:
        Union[Any, List[str]]: 
            - If field is specified: The analysis data for that field
            - If field is None: List of all available field names in the summary
    
    Example:
        >>> readsummary('field_hint_map')  # Returns field hints for scraping
        >>> readsummary()  # Returns ['repeats', 'field_hint_map', 'text_patterns', ...]
    """
    try:
        # Handle null/empty field properly
        if field is None or field == "null" or field == "":
            result = list(summary.keys())
        else:
            result = summary.get(field, f"Field '{field}' not found in summary")
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error reading summary: {str(e)}", "status": "error"}

@mcp.tool
def readHTML(
    start_line: Optional[int] = None,
    end_line: Optional[int] = None,
    css_selector: Optional[str] = None,
    text_search: Optional[str] = None,
    attribute_filter: Optional[str] = None,
    context_lines: int = 10,
    max_results: int = 5
) -> Dict[str, Any]:
    """
    Read HTML content with multiple search modes for robust and targeted HTML inspection.
    
    This enhanced function allows you to find and read HTML content using multiple approaches:
    1. Line-based reading 
    2. CSS selector search (find elements matching CSS selector)
    3. Text content search (find elements containing specific text)
    4. Attribute search (find elements with specific attributes)
    
    Args:
        start_line (Optional[int]): Starting line number for line-based reading (1-based)
        end_line (Optional[int]): Ending line number for line-based reading (inclusive)
        css_selector (Optional[str]): CSS selector to find elements (e.g., 'div.price', 'a.product-link')
        text_search (Optional[str]): Text content to search for (case-insensitive)
        attribute_filter (Optional[str]): Filter by attribute (format: 'attr=value' or 'attr')
        context_lines (int): Number of lines to include before/after matches (default: 10)
        max_results (int): Maximum number of results to return (default: 5)
    
    Returns:
        Dict[str, Any]: HTML content with metadata:
            - content: The matched HTML content
            - matches_found: Number of matches found
            - match_positions: Line numbers where matches occurred
            - search_mode: The mode used ('line_range', 'css_selector', 'text_search', 'attribute')
            - status: 'success' or 'error'
    
    Examples:
        # Line-based reading (backward compatible)
        >>> readHTML(1, 20)  # Returns first 20 lines
        
        # CSS selector search
        >>> readHTML(css_selector='div.Nx9bqj')  # Find all elements with class Nx9bqj
        >>> readHTML(css_selector='a[href*="/p/"]')  # Find all product links
        
        # Text search
        >>> readHTML(text_search='â‚¹1,299')  # Find lines containing this price
        >>> readHTML(text_search='add to cart', context_lines=20)  # Find with more context
        
        # Attribute search
        >>> readHTML(attribute_filter='data-id=123')  # Find element with specific data-id
        >>> readHTML(attribute_filter='class')  # Find all elements with class attribute
        
        # Combined search
        >>> readHTML(css_selector='div.price', text_search='â‚¹', max_results=3)
    """
    try:
        from bs4 import BeautifulSoup
        from lxml import html as lxml_html
        
        if not html_content:
            return {"content": "No HTML content available", "status": "error"}
        
        # Determine search mode
        search_mode = "line_range"
        if css_selector:
            search_mode = "css_selector"
        elif text_search:
            search_mode = "text_search"
        elif attribute_filter:
            search_mode = "attribute"
        
        # Mode 1: Line-based reading (original functionality)
        if start_line is not None and end_line is not None:
            lines = html_content.split('\n')
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)
            result = '\n'.join(lines[start_idx:end_idx])
            
            return {
                "content": result,
                "lines_retrieved": end_idx - start_idx,
                "search_mode": search_mode,
                "status": "success"
            }
        
        # Mode 2-4: Content-based search using BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        matches = []
        match_positions = []
        
        # Mode 2: CSS selector search
        if search_mode == "css_selector":
            elements = soup.select(css_selector)
            matches = [str(elem) for elem in elements[:max_results]]
            
            # Find line positions for matched elements
            html_lines = html_content.split('\n')
            for elem in elements[:max_results]:
                elem_str = str(elem)[:100]  # First 100 chars for matching
                for i, line in enumerate(html_lines):
                    if elem_str[:50] in line:
                        match_positions.append(i + 1)
                        break
        
        # Mode 3: Text search
        elif search_mode == "text_search":
            html_lines = html_content.split('\n')
            for i, line in enumerate(html_lines):
                if text_search.lower() in line.lower():
                    # Include context lines before and after
                    start_idx = max(0, i - context_lines)
                    end_idx = min(len(html_lines), i + context_lines + 1)
                    context = '\n'.join(html_lines[start_idx:end_idx])
                    matches.append(f"Line {i + 1}:\n{context}")
                    match_positions.append(i + 1)
                    
                    if len(matches) >= max_results:
                        break
        
        # Mode 4: Attribute search
        elif search_mode == "attribute":
            # Parse attribute filter (format: 'attr=value' or 'attr')
            parts = attribute_filter.split('=')
            attr_name = parts[0].strip()
            attr_value = parts[1].strip() if len(parts) > 1 else None
            
            if attr_value:
                elements = soup.find_all(attrs={attr_name: attr_value})
            else:
                elements = soup.find_all(attrs={attr_name: True})
            
            matches = [str(elem) for elem in elements[:max_results]]
            
            # Find line positions
            html_lines = html_content.split('\n')
            for elem in elements[:max_results]:
                elem_str = str(elem)[:100]
                for i, line in enumerate(html_lines):
                    if elem_str[:50] in line:
                        match_positions.append(i + 1)
                        break
        
        # Format results
        if matches:
            if search_mode == "text_search":
                result = '\n\n---\n\n'.join(matches)
            else:
                result = '\n\n---\n\n'.join(matches)
            
            return {
                "content": result,
                "matches_found": len(matches),
                "match_positions": match_positions[:max_results],
                "search_mode": search_mode,
                "query": css_selector or text_search or attribute_filter,
                "status": "success"
            }
        else:
            return {
                "content": f"No matches found for: {css_selector or text_search or attribute_filter}",
                "matches_found": 0,
                "match_positions": [],
                "search_mode": search_mode,
                "status": "success"
            }
    
    except Exception as e:
        return {"error": f"Error reading HTML: {str(e)}", "status": "error"}

# ============================================================================
# SELECTOR CONFIGURATION TOOLS
# ============================================================================

@mcp.tool
def get_available_fields() -> Dict[str, Any]:
    """
    Get the list of available fields for selector configuration.
    
    This function helps the LLM understand what fields are available
    for the set_selector function, preventing invalid field name errors.
    
    Returns:
        Dict[str, Any]: Available fields with descriptions
        
    Example:
        >>> fields = get_available_fields()
        >>> print(fields['available_fields'])
        ['product_container', 'name', 'current_price', ...]
    """
    try:
        global selector_template
        result = {
            "available_fields": list(selector_template.keys()),
            "field_descriptions": {
                "product_container": "Container element that wraps each product",
                "name": "Product name/title", 
                "current_price": "Current selling price",
                "original_price": "Original/MRP price (optional)",
                "rating": "Product rating (1-5 stars)",
                "reviews": "Number of reviews/ratings",
                "discount": "Discount percentage or amount",
                "offers": "Special offers or coupons"
            },
            "usage_example": "Use set_selector('name', 'css', ['h1.title', 'a.product-link']) to set name selectors",
            "status": "success"
        }
        return result
    except Exception as e:
        return {"error": f"Error getting available fields: {str(e)}", "status": "error"}

@mcp.tool
def set_selector(field: str, selector_type: str, selectors: List[str]) -> Dict[str, Any]:
    """
    Set selector configuration for a specific field in the global template.
    
    This function allows the LLM to incrementally build the selector configuration
    by filling in specific fields one at a time, avoiding token limit issues
    and reducing the chance of format errors.
    
    Args:
        field (str): The field name to set. MUST be one of these exact values:
                    - 'product_container': Container element that wraps each product
                    - 'name': Product name/title
                    - 'current_price': Current selling price
                    - 'original_price': Original/MRP price (optional)
                    - 'rating': Product rating (1-5 stars)
                    - 'reviews': Number of reviews/ratings
                    - 'discount': Discount percentage or amount
                    - 'offers': Special offers or coupons
        selector_type (str): The type of selector. MUST be one of: 'css', 'xpath', 'regex'
        selectors (List[str]): List of selector strings for the field. Must be non-empty.
    
    Returns:
        Dict[str, Any]: The current state of the selector template after the update
        
    Example:
        >>> template = set_selector("name", "css", ['img[alt]', 'a[class*="wjcEIp"]'])
        >>> print(template['name'])
        {'type': 'css', 'selectors': ['img[alt]', 'a[class*="wjcEIp"]']}
        
        >>> template = set_selector("current_price", "css", ['div[class*="Nx9bqj"]'])
        >>> print(template['current_price'])
        {'type': 'css', 'selectors': ['div[class*="Nx9bqj"]']}
        
    Note:
        Available fields are: product_container, name, current_price, original_price, 
        rating, reviews, discount, offers. Do NOT use fields like 'image_url', 
        'product_url', 'description', etc. as they are not supported.
    """
    try:
        global selector_template
        
        # Validate field name
        if field not in selector_template:
            available_fields = list(selector_template.keys())
            return {
                "error": f"Invalid field '{field}'. Available fields: {available_fields}",
                "suggestion": f"Use one of these exact field names: {', '.join(available_fields)}",
                "field_descriptions": {
                    "product_container": "Container element that wraps each product",
                    "name": "Product name/title", 
                    "current_price": "Current selling price",
                    "original_price": "Original/MRP price (optional)",
                    "rating": "Product rating (1-5 stars)",
                    "reviews": "Number of reviews/ratings",
                    "discount": "Discount percentage or amount",
                    "offers": "Special offers or coupons"
                },
                "status": "error"
            }
        
        # Validate selector type
        valid_types = ['css', 'xpath', 'regex']
        if selector_type not in valid_types:
            return {
                "error": f"Invalid selector type '{selector_type}'. Valid types: {valid_types}",
                "status": "error"
            }
        
        # Validate selectors list
        if not isinstance(selectors, list) or not selectors:
            return {
                "error": "Selectors must be a non-empty list of strings",
                "status": "error"
            }
        
        # Update the template
        selector_template[field] = {
            'type': selector_type,
            'selectors': selectors
        }
        
        result = {
            "message": f"Successfully set {field} selector",
            "field": field,
            "type": selector_type,
            "selectors": selectors,
            "template": selector_template,
            "status": "success"
        }
        return result
    except Exception as e:
        return {"error": f"Error setting selector: {str(e)}", "status": "error"}

@mcp.tool
def create_scraper(platform_name: str) -> Dict[str, str]:
    """
    Create a GenericPlatformScraper instance and store it in global variable.
    
    This function initializes a scraper that can extract product data from HTML content
    using the configured selector template. The scraper object is stored in the global variable
    GenericPlatformScraperObj for use by other tools.
    
    Args:
        platform_name (str): Name of the platform/site for identification and logging
    
    Returns:
        Dict[str, str]: Success message confirming scraper object creation and storage
        
    Side Effects:
        - Updates global variable GenericPlatformScraperObj with the created scraper instance
        - The scraper object is now available for use by other analysis and validation tools
        
    Note:
        The scraper is initialized but not yet executed. The object is stored globally
        and can be used by other tools to perform operations on the data.
    """
    try:
        global GenericPlatformScraperObj, html_content, selector_template
        
        if not html_content:
            return {"error": "No HTML content available. Please call get_html() first.", "status": "error"}
        
        GenericPlatformScraperObj = GenericPlatformScraper(html_content, selector_template, platform_name)
        return {"message": "Successfully created GenericPlatformScraper object and stored in global variable", "status": "success"}
    except Exception as e:
        return {"error": f"Error creating scraper: {str(e)}", "status": "error"}

# ============================================================================
# DATA EXTRACTION TOOLS
# ============================================================================

@mcp.tool
def extract_products() -> Dict[str, Any]:
    """
    Extract product data using the configured scraper.
    
    This function uses the globally stored GenericPlatformScraperObj to extract
    product data from the HTML content using the configured selectors.
    
    Returns:
        Dict[str, Any]: Extracted products data with metadata
        
    Example:
        >>> result = extract_products()
        >>> print(f"Extracted {result['product_count']} products")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        products = GenericPlatformScraperObj.scrape()
        
        return {
            "products": products,
            "product_count": len(products),
            "status": "success"
        }
    except Exception as e:
        return {"error": f"Error extracting products: {str(e)}", "status": "error"}

# ============================================================================
# DATA INSPECTION TOOLS
# ============================================================================

@mcp.tool
def inspect_extracted_data(
    field: Optional[str] = None,
    show_na_only: bool = False,
    limit: int = 10,
    sample_mode: bool = False
) -> Dict[str, Any]:
    """
    Inspect the actual data extracted by the current selectors for detailed analysis.
    
    This tool provides complete visibility into what was actually extracted, allowing
    the agent to understand extraction quality, identify N/A values, and debug selector issues.
    
    Args:
        field (Optional[str]): Filter by specific field name. If None, shows all products
                               Options: 'name', 'current_price', 'original_price', 
                                       'rating', 'reviews', 'discount', 'offers'
        show_na_only (bool): If True, only show products where the specified field is N/A
        limit (int): Maximum number of products to return (default: 10)
        sample_mode (bool): If True, returns representative sample instead of first N products
    
    Returns:
        Dict[str, Any]: Inspection results including:
            - products: List of extracted products with their data
            - total_products: Total number of products extracted
            - field_stats: Statistics for each field (N/A count, valid count, samples)
            - problem_fields: Fields with high N/A rates
            - sample_data: Sample extracted values for each field
            - status: 'success' or 'error'
    
    Examples:
        # See all extracted products
        >>> inspect_extracted_data(limit=5)
        
        # Check only the name field
        >>> inspect_extracted_data(field='name')
        
        # Find products where rating is N/A
        >>> inspect_extracted_data(field='rating', show_na_only=True)
        
        # Get statistics about all fields
        >>> inspect_extracted_data(sample_mode=True)
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        if not hasattr(GenericPlatformScraperObj, 'products') or not GenericPlatformScraperObj.products:
            return {"error": "No products extracted yet. Please call extract_products() first.", "status": "error"}
        
        products = GenericPlatformScraperObj.products
        
        # Calculate field statistics
        field_stats = {}
        available_fields = ['name', 'current_price', 'original_price', 'rating', 'reviews', 'discount', 'offers']
        
        for field_name in available_fields:
            na_count = 0
            valid_count = 0
            samples = []
            
            for product in products:
                value = product.get(field_name, 'N/A')
                if value == 'N/A' or value == 'N/A' or value is None or value == '':
                    na_count += 1
                else:
                    valid_count += 1
                    if len(samples) < 3:  # Collect up to 3 samples
                        samples.append(value)
            
            total = len(products)
            na_percentage = (na_count / total * 100) if total > 0 else 0
            
            field_stats[field_name] = {
                "na_count": na_count,
                "valid_count": valid_count,
                "na_percentage": f"{na_percentage:.1f}%",
                "status": "GOOD" if na_percentage < 20 else "NEEDS_IMPROVEMENT" if na_percentage < 50 else "POOR",
                "samples": samples
            }
        
        # Filter products based on parameters
        filtered_products = products
        
        if field and show_na_only:
            filtered_products = [p for p in products if p.get(field, 'N/A') == 'N/A']
        elif field and not show_na_only:
            # Just filter to show products, keeping all fields
            pass
        
        # Apply limit
        if sample_mode and len(filtered_products) > limit:
            # Select representative sample
            indices = [int(i * len(filtered_products) / limit) for i in range(limit)]
            filtered_products = [filtered_products[i] for i in indices]
        else:
            filtered_products = filtered_products[:limit]
        
        # Identify problem fields (high N/A rate)
        problem_fields = []
        for field_name, stats in field_stats.items():
            if float(stats['na_percentage'].replace('%', '')) >= 50:
                problem_fields.append({
                    'field': field_name,
                    'na_percentage': stats['na_percentage'],
                    'status': stats['status']
                })
        
        return {
            "products": filtered_products,
            "total_products": len(products),
            "displayed_products": len(filtered_products),
            "field_stats": field_stats,
            "problem_fields": problem_fields,
            "summary": f"Showing {len(filtered_products)} of {len(products)} products",
            "status": "success"
        }
    
    except Exception as e:
        return {"error": f"Error inspecting extracted data: {str(e)}", "status": "error"}

# ============================================================================
# ANALYSIS AND VALIDATION TOOLS
# ============================================================================

@mcp.tool
def get_selector_performance() -> Dict[str, Any]:
    """
    Analyze selector performance and data extraction success rates.
    
    This function evaluates how well the selectors are performing by calculating
    success rates for each field (name, price, rating, etc.) and providing
    overall performance metrics. Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Performance metrics including:
            - total_products (int): Number of products scraped
            - site (str): Site name
            - field_performance (Dict): Success rates for each field
            - overall_success_rate (str): Overall extraction success rate
            - scraped_at (str): Timestamp of scraping
    
    Example:
        >>> performance = get_selector_performance()
        >>> print(f"Overall success: {performance['overall_success_rate']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            total_products = len(analyzer.df)
            field_performance = {}
            for field in ['name', 'current_price', 'original_price', 'rating', 'reviews', 'discount', 'offers']:
                if field in analyzer.df.columns:
                    valid_count = int(analyzer.df[field].apply(analyzer._is_valid_data).sum())
                    success_rate = (valid_count / total_products) * 100 if total_products > 0 else 0
                    field_performance[field] = {
                        "success_rate": f"{success_rate:.1f}%",
                        "valid_extractions": valid_count,
                        "total_attempts": total_products,
                        "failed_extractions": total_products - valid_count,
                        "status": "GOOD" if success_rate >= 80 else "NEEDS_IMPROVEMENT" if success_rate >= 50 else "POOR"
                    }
            result = {
                "total_products": total_products,
                "site": analyzer.df['site'].iloc[0] if not analyzer.df.empty else "unknown",
                "scraped_at": analyzer.df['scraped_at'].iloc[0] if not analyzer.df.empty else None,
                "field_performance": field_performance,
                "overall_success_rate": f"{(sum(int(field_performance[f]['valid_extractions']) for f in field_performance) / (len(field_performance) * total_products) * 100):.1f}%" if total_products > 0 else "0%"
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error analyzing performance: {str(e)}", "status": "error"}

@mcp.tool
def validate_price_selectors() -> Dict[str, Any]:
    """
    Validate price selector performance and data quality.
    
    This function specifically analyzes price extraction performance, checking
    for valid numeric values and providing recommendations for improvement.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Price validation results including:
            - current_price_selector (Dict): Current price validation metrics
            - original_price_selector (Dict): Original price validation metrics
            Each contains: success_rate, valid_extractions, status, sample_data, recommendation
    
    Example:
        >>> price_validation = validate_price_selectors()
        >>> print(f"Current price success: {price_validation['current_price_selector']['success_rate']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Simple test without any imports from data_analysis.py
        result = {
            "test": "Price validation function is working",
            "scraper_type": str(type(GenericPlatformScraperObj)),
            "products_count": len(GenericPlatformScraperObj.products) if hasattr(GenericPlatformScraperObj, 'products') else 0
        }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error validating price selectors: {str(e)}", "status": "error"}

@mcp.tool
def validate_rating_selectors() -> Dict[str, Any]:
    """
    Validate rating selector performance and data quality.
    
    This function analyzes rating extraction performance, checking for valid
    rating values (0-5 range) and providing quality metrics.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Rating validation results including:
            - rating_selector (Dict): Rating validation metrics containing:
                - success_rate (str): Percentage of successful extractions
                - valid_extractions (int): Number of valid ratings found
                - valid_range_extractions (int): Ratings in valid 0-5 range
                - status (str): "GOOD", "NEEDS_IMPROVEMENT", or "POOR"
                - sample_data (List[str]): Sample extracted ratings
                - recommendation (str): Improvement suggestions
    
    Example:
        >>> rating_validation = validate_rating_selectors()
        >>> print(f"Rating status: {rating_validation['rating_selector']['status']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            total_products = len(analyzer.df)
            # Rating validation
            rating_valid = int(analyzer.df['rating'].apply(analyzer._is_valid_data).sum())
            rating_numeric = int(analyzer.df['rating'].apply(analyzer._extract_numeric_value).notna().sum())
            # Sample extracted ratings for validation
            rating_samples = analyzer.df[analyzer.df['rating'].apply(analyzer._is_valid_data)]['rating'].head(5).tolist()
            # Check if ratings are in valid range (0-5)
            valid_ratings = 0
            if rating_numeric > 0:
                numeric_ratings = analyzer.df['rating'].apply(analyzer._extract_numeric_value).dropna()
                valid_ratings = int(len(numeric_ratings[(numeric_ratings >= 0) & (numeric_ratings <= 5)]))
            result = {
                "rating_selector": {
                    "success_rate": f"{(rating_valid / total_products) * 100:.1f}%",
                    "valid_extractions": rating_valid,
                    "numeric_extractions": rating_numeric,
                    "valid_range_extractions": valid_ratings,
                    "status": "GOOD" if rating_valid >= total_products * 0.8 else "NEEDS_IMPROVEMENT" if rating_valid >= total_products * 0.5 else "POOR",
                    "sample_data": rating_samples,
                    "recommendation": "Selector working well" if rating_valid >= total_products * 0.8 else "Consider improving rating selectors"
                }
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error validating rating selectors: {str(e)}", "status": "error"}

@mcp.tool
def validate_review_selectors() -> Dict[str, Any]:
    """
    Validate review selector performance and data quality.
    
    This function analyzes review count extraction performance, checking for
    valid numeric review counts and providing quality metrics.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Review validation results including:
            - review_selector (Dict): Review validation metrics containing:
                - success_rate (str): Percentage of successful extractions
                - valid_extractions (int): Number of valid reviews found
                - numeric_extractions (int): Reviews with numeric values
                - status (str): "GOOD", "NEEDS_IMPROVEMENT", or "POOR"
                - sample_data (List[str]): Sample extracted review counts
                - recommendation (str): Improvement suggestions
    
    Example:
        >>> review_validation = validate_review_selectors()
        >>> print(f"Review success: {review_validation['review_selector']['success_rate']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            total_products = len(analyzer.df)
            # Review validation
            review_valid = int(analyzer.df['reviews'].apply(analyzer._is_valid_data).sum())
            review_numeric = int(analyzer.df['reviews'].apply(analyzer._extract_numeric_value).notna().sum())
            # Sample extracted reviews for validation
            review_samples = analyzer.df[analyzer.df['reviews'].apply(analyzer._is_valid_data)]['reviews'].head(5).tolist()
            result = {
                "review_selector": {
                    "success_rate": f"{(review_valid / total_products) * 100:.1f}%",
                    "valid_extractions": review_valid,
                    "numeric_extractions": review_numeric,
                    "status": "GOOD" if review_valid >= total_products * 0.8 else "NEEDS_IMPROVEMENT" if review_valid >= total_products * 0.5 else "POOR",
                    "sample_data": review_samples,
                    "recommendation": "Selector working well" if review_valid >= total_products * 0.8 else "Consider improving review selectors"
                }
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error validating review selectors: {str(e)}", "status": "error"}

@mcp.tool
def validate_name_selectors() -> Dict[str, Any]:
    """
    Validate name selector performance and data quality.
    
    This function analyzes product name extraction performance, checking for
    reasonable name lengths and providing quality metrics.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Name validation results including:
            - name_selector (Dict): Name validation metrics containing:
                - success_rate (str): Percentage of successful extractions
                - valid_extractions (int): Number of valid names found
                - reasonable_length_extractions (int): Names with reasonable length (5-200 chars)
                - status (str): "GOOD", "NEEDS_IMPROVEMENT", or "POOR"
                - sample_data (List[str]): Sample extracted product names
                - recommendation (str): Improvement suggestions
    
    Example:
        >>> name_validation = validate_name_selectors()
        >>> print(f"Name status: {name_validation['name_selector']['status']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            total_products = len(analyzer.df)
            # Name validation
            name_valid = int(analyzer.df['name'].apply(analyzer._is_valid_data).sum())
            # Sample extracted names for validation
            name_samples = analyzer.df[analyzer.df['name'].apply(analyzer._is_valid_data)]['name'].head(5).tolist()
            # Check name length (should be reasonable)
            valid_length_names = 0
            if name_valid > 0:
                valid_names = analyzer.df[analyzer.df['name'].apply(analyzer._is_valid_data)]['name']
                valid_length_names = int(len(valid_names[(valid_names.str.len() >= 5) & (valid_names.str.len() <= 200)]))
            result = {
                "name_selector": {
                    "success_rate": f"{(name_valid / total_products) * 100:.1f}%",
                    "valid_extractions": name_valid,
                    "reasonable_length_extractions": valid_length_names,
                    "status": "GOOD" if name_valid >= total_products * 0.8 else "NEEDS_IMPROVEMENT" if name_valid >= total_products * 0.5 else "POOR",
                    "sample_data": name_samples,
                    "recommendation": "Selector working well" if name_valid >= total_products * 0.8 else "Consider improving name selectors"
                }
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error validating name selectors: {str(e)}", "status": "error"}

@mcp.tool
def get_selector_validation_report() -> Dict[str, Any]:
    """
    Get comprehensive selector validation report.
    
    This function provides a complete validation report combining all field
    validations with overall health scoring and recommendations.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Comprehensive validation report including:
            - overall_selector_health (Dict): Overall health score and status
            - field_performance (Dict): Performance metrics for each field
            - detailed_validations (Dict): Detailed validation results for each field type
            - critical_issues (List[str]): List of critical problems found
            - recommendations (List[str]): List of improvement recommendations
            - analysis_timestamp (str): When the analysis was performed
    
    Example:
        >>> report = get_selector_validation_report()
        >>> print(f"Overall health: {report['overall_selector_health']['status']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            total_products = len(analyzer.df)
            # Calculate field performance
            field_performance = {}
            for field in ['name', 'current_price', 'original_price', 'rating', 'reviews', 'discount', 'offers']:
                if field in analyzer.df.columns:
                    valid_count = int(analyzer.df[field].apply(analyzer._is_valid_data).sum())
                    success_rate = (valid_count / total_products) * 100 if total_products > 0 else 0
                    field_performance[field] = {
                        "success_rate": f"{success_rate:.1f}%",
                        "valid_extractions": valid_count,
                        "total_attempts": total_products,
                        "failed_extractions": total_products - valid_count,
                        "status": "GOOD" if success_rate >= 80 else "NEEDS_IMPROVEMENT" if success_rate >= 50 else "POOR"
                    }
            # Calculate overall selector health
            field_scores = []
            for field, data in field_performance.items():
                success_rate = float(data['success_rate'].replace('%', ''))
                field_scores.append(success_rate)
            overall_score = sum(field_scores) / len(field_scores) if field_scores else 0
            # Generate recommendations
            recommendations = []
            critical_issues = []
            for field, data in field_performance.items():
                success_rate = float(data['success_rate'].replace('%', ''))
                if success_rate < 50:
                    critical_issues.append(f"{field} selector is failing ({success_rate:.1f}% success rate)")
                elif success_rate < 80:
                    recommendations.append(f"Consider improving {field} selectors ({success_rate:.1f}% success rate)")
            result = {
                "overall_selector_health": {
                    "score": f"{overall_score:.1f}%",
                    "status": "EXCELLENT" if overall_score >= 90 else "GOOD" if overall_score >= 80 else "NEEDS_IMPROVEMENT" if overall_score >= 60 else "POOR",
                    "total_products": total_products,
                    "site": analyzer.df['site'].iloc[0] if not analyzer.df.empty else "unknown"
                },
                "field_performance": field_performance,
                "critical_issues": critical_issues,
                "recommendations": recommendations,
                "analysis_timestamp": "2024-01-01T00:00:00"  # Simplified timestamp
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error getting validation report: {str(e)}", "status": "error"}

@mcp.tool
def get_selector_improvement_suggestions() -> Dict[str, Any]:
    """
    Get specific suggestions for improving selectors.
    
    This function analyzes selector performance and provides prioritized
    suggestions for improving data extraction quality.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Improvement suggestions organized by priority:
            - high_priority (List[Dict]): Critical issues requiring immediate attention
            - medium_priority (List[Dict]): Issues that should be addressed soon
            - low_priority (List[Dict]): Minor improvements for optimization
            - sample_failures (Dict): Sample failed extractions for each field
            Each suggestion contains: field, issue, suggestion
    
    Example:
        >>> suggestions = get_selector_improvement_suggestions()
        >>> for issue in suggestions['high_priority']:
        ...     print(f"Fix {issue['field']}: {issue['suggestion']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            suggestions = {
                "high_priority": [],
                "medium_priority": [],
                "low_priority": [],
                "sample_failures": {}
            }
            # Analyze each field and provide specific suggestions
            for field in ['name', 'current_price', 'original_price', 'rating', 'reviews', 'discount', 'offers']:
                if field in analyzer.df.columns:
                    valid_count = int(analyzer.df[field].apply(analyzer._is_valid_data).sum())
                    total_count = len(analyzer.df)
                    success_rate = float((valid_count / total_count) * 100)
                    # Get sample failures
                    failures = analyzer.df[~analyzer.df[field].apply(analyzer._is_valid_data)][field].head(3).tolist()
                    suggestions["sample_failures"][field] = failures
                    if success_rate < 50:
                        suggestions["high_priority"].append({
                            "field": field,
                            "issue": f"Only {success_rate:.1f}% success rate",
                            "suggestion": f"Review and update {field} selectors - consider adding more fallback selectors or changing selector strategy"
                        })
                    elif success_rate < 80:
                        suggestions["medium_priority"].append({
                            "field": field,
                            "issue": f"{success_rate:.1f}% success rate",
                            "suggestion": f"Consider adding fallback selectors for {field} to improve reliability"
                        })
                    elif success_rate < 95:
                        suggestions["low_priority"].append({
                            "field": field,
                            "issue": f"{success_rate:.1f}% success rate",
                            "suggestion": f"Minor improvements possible for {field} selectors"
                        })
            result = suggestions
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error getting improvement suggestions: {str(e)}", "status": "error"}

@mcp.tool
def get_comprehensive_selector_analysis() -> Dict[str, Any]:
    """
    Get comprehensive selector validation analysis.
    
    This function combines all analysis functions into a single comprehensive
    report providing complete insights into selector performance.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Complete analysis including:
            - selector_performance (Dict): Overall performance metrics
            - price_validation (Dict): Price-specific validation results
            - rating_validation (Dict): Rating-specific validation results
            - review_validation (Dict): Review-specific validation results
            - name_validation (Dict): Name-specific validation results
            - validation_report (Dict): Comprehensive validation report
            - improvement_suggestions (Dict): Prioritized improvement suggestions
            - analysis_timestamp (str): Analysis timestamp
    
    Example:
        >>> analysis = get_comprehensive_selector_analysis()
        >>> print(f"Analysis completed at: {analysis['analysis_timestamp']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Use SelectorAnalyzer directly to avoid circular imports
        analyzer = SelectorAnalyzer(GenericPlatformScraperObj)
        if analyzer.df.empty:
            result = {"error": "No data available for analysis"}
        else:
            # Simplified comprehensive analysis
            result = {
                "selector_performance": {"message": "Use get_selector_performance() for detailed performance metrics"},
                "price_validation": {"message": "Use validate_price_selectors() for detailed price validation"},
                "rating_validation": {"message": "Use validate_rating_selectors() for detailed rating validation"},
                "review_validation": {"message": "Use validate_review_selectors() for detailed review validation"},
                "name_validation": {"message": "Use validate_name_selectors() for detailed name validation"},
                "validation_report": {"message": "Use get_selector_validation_report() for detailed validation report"},
                "improvement_suggestions": {"message": "Use get_selector_improvement_suggestions() for detailed suggestions"},
                "analysis_timestamp": "2024-01-01T00:00:00"
            }
        return {"data": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error getting comprehensive analysis: {str(e)}", "status": "error"}

# ============================================================================
# EXPORT AND SAVE TOOLS
# ============================================================================

@mcp.tool
def export_selector_analysis_to_json(filename: Optional[str] = None) -> Dict[str, str]:
    """
    Export comprehensive selector analysis to JSON file.
    
    This function saves the complete selector analysis to a JSON file for
    later review, sharing, or integration with other tools.
    Uses the globally stored GenericPlatformScraperObj.
    
    Args:
        filename (Optional[str]): Output filename. If None, auto-generates with timestamp
    
    Returns:
        Dict[str, str]: Path to the exported JSON file
    
    Example:
        >>> json_file = export_selector_analysis_to_json("analysis.json")
        >>> print(f"Analysis saved to: {json_file}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {"error": "No scraper object available. Please call create_scraper() first.", "status": "error"}
        
        # Simplified export function
        if not filename:
            filename = "selector_analysis_export.json"
        result = filename  # Just return the filename for now
        return {"file_path": result, "status": "success"}
    except Exception as e:
        return {"error": f"Error exporting analysis: {str(e)}", "status": "error"}

@mcp.tool
def save_to_database() -> Dict[str, Any]:
    """
    Save scraped data to Supabase database.
    
    This function saves the scraped products to a Supabase database table.
    It requires SUPABASE_URL and SUPABASE_KEY environment variables to be set.
    Uses the globally stored GenericPlatformScraperObj.
    
    Returns:
        Dict[str, Any]: Save operation result including:
            - success (bool): Whether the save operation was successful
            - saved_count (int): Number of products saved to database
            - table_name (str): Name of the database table used
            - site (str): Site name from scraper
            - scraped_at (str): Timestamp when data was scraped
            - message (str): Success message
            - error (str): Error message if operation failed
    
    Environment Variables Required:
        - SUPABASE_URL: Your Supabase project URL
        - SUPABASE_KEY: Your Supabase API key
        - SUPABASE_TABLE_NAME: Database table name (optional, defaults to 'scraped_products')
    
    Example:
        >>> result = save_to_database()
        >>> if result['success']:
        ...     print(f"Saved {result['saved_count']} products to {result['table_name']}")
        >>> else:
        ...     print(f"Error: {result['error']}")
    """
    try:
        global GenericPlatformScraperObj
        
        if GenericPlatformScraperObj is None:
            return {
                "success": False,
                "saved_count": 0,
                "error": "No scraper object available. Please call create_scraper() first.",
                "status": "error"
            }
        
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')
        table_name = os.getenv('SUPABASE_TABLE_NAME', 'scraped_products')
        
        if not supabase_url or not supabase_key:
            return {
                "success": False,
                "saved_count": 0,
                "error": "Supabase credentials not found in environment variables. Please set SUPABASE_URL and SUPABASE_KEY.",
                "status": "error"
            }
        
        result = GenericPlatformScraperObj.save_to_supabase(supabase_url, supabase_key, table_name)
        result["status"] = "success"
        return result
    except Exception as e:
        return {
            "success": False,
            "saved_count": 0,
            "error": f"Error saving to database: {str(e)}",
            "status": "error"
        }

# ============================================================================
# UTILITY TOOLS
# ============================================================================

@mcp.tool
def get_current_state() -> Dict[str, Any]:
    """
    Get the current state of all global variables.
    
    This function provides a snapshot of the current state including:
    - HTML content status
    - Analysis summary status
    - Selector template status
    - Scraper object status
    
    Returns:
        Dict[str, Any]: Current state information
    """
    try:
        global html_content, summary, selector_template, GenericPlatformScraperObj
        
        return {
            "html_content_available": bool(html_content),
            "html_content_length": len(html_content) if html_content else 0,
            "summary_available": bool(summary),
            "summary_fields": list(summary.keys()) if summary else [],
            "selector_template": selector_template,
            "scraper_available": GenericPlatformScraperObj is not None,
            "status": "success"
        }
    except Exception as e:
        return {"error": f"Error getting current state: {str(e)}", "status": "error"}

@mcp.tool
def reset_state() -> Dict[str, str]:
    """
    Reset all global variables to their initial state.
    
    This function clears all global variables and resets them to their
    initial empty state. Useful for starting a new scraping session.
    
    Returns:
        Dict[str, str]: Success message
    """
    try:
        global html_content, summary, GenericPlatformScraperObj, selector_template
        
        html_content = ""
        summary = {}
        GenericPlatformScraperObj = None
        
        # Reset selector template to empty state
        for field in selector_template:
            selector_template[field] = {
                'type': None,
                'selectors': []
            }
        
        return {"message": "Successfully reset all global variables to initial state", "status": "success"}
    except Exception as e:
        return {"error": f"Error resetting state: {str(e)}", "status": "error"}

# ============================================================================
# RUN THE MCP SERVER
# ============================================================================

if __name__ == "__main__":
    print("ðŸš€ Starting Web Scraping Bot MCP Server...")
    print("=" * 60)
    print("Available tools:")
    print("- get_html: Fetch and analyze HTML content")
    print("- readsummary: Read analysis summary data")
    print("- readHTML: Read HTML by lines, CSS selector, text search, or attributes")
    print("  Examples:")
    print("    â€¢ readHTML(1, 20) - Line-based reading")
    print("    â€¢ readHTML(css_selector='div.price') - Find elements by CSS")
    print("    â€¢ readHTML(text_search='â‚¹1,299') - Search for text")
    print("    â€¢ readHTML(attribute_filter='data-id=123') - Filter by attribute")
    print("- get_available_fields: Get available selector fields")
    print("- set_selector: Set selector configuration")
    print("- create_scraper: Create scraper instance")
    print("- extract_products: Extract product data")
    print("- inspect_extracted_data: Inspect actual extracted data and field statistics")
    print("- get_selector_performance: Analyze selector performance")
    print("- validate_price_selectors: Validate price selectors")
    print("- validate_rating_selectors: Validate rating selectors")
    print("- validate_review_selectors: Validate review selectors")
    print("- validate_name_selectors: Validate name selectors")
    print("- get_selector_validation_report: Get comprehensive validation report")
    print("- get_selector_improvement_suggestions: Get improvement suggestions")
    print("- get_comprehensive_selector_analysis: Get complete analysis")
    print("- export_selector_analysis_to_json: Export analysis to JSON")
    print("- save_to_database: Save data to Supabase")
    print("- get_current_state: Get current state information")
    print("- reset_state: Reset all global variables")
    print("=" * 60)
    
    mcp.run()
